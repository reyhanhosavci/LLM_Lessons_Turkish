# LLM_Lessons_Turkish
These implementations are based on the YouTube tutorial series LLM Lessons (https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu) and the source materials shared publicly by the original author on Google Drive.
# LLM Lessons (TÃ¼rkÃ§e AÃ§Ä±klamalÄ± Kodlar)  
**LLM Lessons â€” Turkish Annotated Implementations**

---

## ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e AÃ§Ä±klama

Bu depo, [LLM Lessons YouTube serisi](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu)  
videolarÄ±nda anlatÄ±lan Ã¶rnek kodlarÄ±n **TÃ¼rkÃ§e aÃ§Ä±klamalÄ± versiyonlarÄ±nÄ±** iÃ§ermektedir.  

Kodlar, orijinal iÃ§erik Ã¼reticisinin Google Drive Ã¼zerinden **herkese aÃ§Ä±k olarak paylaÅŸtÄ±ÄŸÄ±** eÄŸitim materyallerine dayanmaktadÄ±r.  
Bu Ã§alÄ±ÅŸma **Ã¶ÄŸrenme, Ã¶ÄŸretme ve araÅŸtÄ±rma amaÃ§lÄ±dÄ±r**; ticari bir kullanÄ±m veya daÄŸÄ±tÄ±m hedeflenmemektedir.  

Her dosyada, orijinal kod yapÄ±sÄ±na sadÄ±k kalÄ±narak **TÃ¼rkÃ§e aÃ§Ä±klamalar, yorum satÄ±rlarÄ± ve aÃ§Ä±klayÄ±cÄ± notlar** eklenmiÅŸtir.  
AmaÃ§, bÃ¼yÃ¼k dil modelleri (LLM) konusunu TÃ¼rkÃ§e olarak daha anlaÅŸÄ±lÄ±r hale getirmektir.

ğŸ“š Ä°Ã§erik BaÅŸlÄ±klarÄ± â€” LLM Lessons Serisi
| Ders              | Dosya AdÄ±                      | BaÅŸlÄ±k                                                 |
| ----------------- | ------------------------------ | ------------------------------------------------------ |
| **Lesson 7**      | `L_7_simple_tokenizer.py`      |  Basit Tokenizer MantÄ±ÄŸÄ± ve UygulamasÄ±               |
| **Lesson 8**      | `L_8_byte_pair_encoding.py`    |  Byte Pair Encoding (BPE) AlgoritmasÄ±                |
| **Lesson 9**      | `L_9_dataloader.py`            |  DataLoader ile Veri YÃ¼kleme ve Batch Ä°ÅŸleme         |
| **Lesson 9 (Ek)** | `L_9_input-output-pairs.py`    |  Inputâ€“Output Ã‡iftlerinin OluÅŸturulmasÄ±              |
| **Lesson 10**     | `L_10_token_embeddings.py`     |  Token Embedding KatmanÄ±                             |
| **Lesson 11**     | `L_11_pos_embeddings.py`       |  Positional Embeddings (Pozisyon Bilgisi)            |
| **Lesson 14**     | `L_14_attention_mech.py`       |  Attention MekanizmasÄ±nÄ±n Temelleri                  |
| **Lesson 15**     | `L_15_self_attention.py`       |  Self-Attention (Kendine Dikkat) YapÄ±sÄ±              |
| **Lesson 16**     | `L_16_causal_attention.py`     |  Causal (Maskeli) Attention                          |
| **Lesson 17**     | `L_17_multi_head_attention.py` |  Multi-Head Attention (Ã‡ok BaÅŸlÄ± Dikkat)             |
| **Lesson 19**     | `L_19_llm_architecture.py`     |  LLM (Large Language Model) Mimarisi Genel YapÄ±sÄ±   |
| **Ek ModÃ¼l**      | `self_attention.py`            |  Self-Attention ModÃ¼l TanÄ±mÄ±                         |
| **Ek ModÃ¼l**      | `multi_head_attention.py`      |  Multi-Head Attention ModÃ¼l TanÄ±mÄ±                   |
| **Ek ModÃ¼l**      | `data_loader.py`               |  Dataset ve DataLoader YardÄ±mcÄ± FonksiyonlarÄ±        |
| **Uygulama**      | `DummyGPTModel.py`             |  GPT Mimarisinin BasitleÅŸtirilmiÅŸ (Dummy) UygulamasÄ± |
| **Veri**          | `the-verdict.txt`              |  EÄŸitim/Test Ä°Ã§in KullanÄ±lan Ã–rnek Metin             |


### âš ï¸ Telif ve Lisans Notu
Bu proje, orijinal video serisinin sahibi tarafÄ±ndan paylaÅŸÄ±lan iÃ§eriklere dayanmaktadÄ±r.  
Orijinal iÃ§erik sahibine ait tÃ¼m haklar saklÄ±dÄ±r.  
Benzer Ã§alÄ±ÅŸmalar yapmak isteyenler iÃ§in kaynak belirtilmesi Ã¶nerilir.

---

## ğŸ‡¬ğŸ‡§ English Description

This repository contains **Turkish-annotated implementations** of the examples from the  
[LLM Lessons YouTube series](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu).

The codes are based on educational materials **publicly shared via Google Drive** by the original author.  
This project is intended **solely for learning, teaching, and research purposes** â€”  
no commercial redistribution or monetization is intended.  

Each file includes **Turkish commentary, code explanations, and docstrings**  
to make Large Language Model (LLM) concepts more accessible to Turkish learners.

ğŸ“š Contents â€” LLM Lessons Serie
| Lesson               | File Name                      | Topic                                                                     |
| -------------------- | ------------------------------ | ------------------------------------------------------------------------- |
| **Lesson 7**         | `L_7_simple_tokenizer.py`      |  Building a Simple Tokenizer                                            |
| **Lesson 8**         | `L_8_byte_pair_encoding.py`    |  Byte Pair Encoding (BPE) Algorithm                                     |
| **Lesson 9**         | `L_9_dataloader.py`            |  Creating a Custom DataLoader and Batching                              |
| **Lesson 9 (Extra)** | `L_9_input-output-pairs.py`    |  Preparing Inputâ€“Output Pairs                                           |
| **Lesson 10**        | `L_10_token_embeddings.py`     |  Token Embedding Layer                                                  |
| **Lesson 11**        | `L_11_pos_embeddings.py`       |  Positional Embeddings (Adding Order Information)                       |
| **Lesson 14**        | `L_14_attention_mech.py`       |  Fundamentals of the Attention Mechanism                                |
| **Lesson 15**        | `L_15_self_attention.py`       |  Self-Attention (Understanding Context Within a Sequence)               |
| **Lesson 16**        | `L_16_causal_attention.py`     |  Causal (Masked) Attention â€” Preventing Information Leakage             |
| **Lesson 17**        | `L_17_multi_head_attention.py` |  Multi-Head Attention Explained                                         |
| **Lesson 19**        | `L_19_llm_architecture.py`     |  LLM Architecture Overview â€” Assembling Model Components               |
| **Module**           | `self_attention.py`            |  Self-Attention Class Implementation                                    |
| **Module**           | `multi_head_attention.py`      |  Multi-Head Attention Class Implementation                              |
| **Module**           | `data_loader.py`               |  Data Preparation Utilities (Dataset & DataLoader)                      |
| **Example**          | `DummyGPTModel.py`             |  Simplified GPT Model (Token + Positional Embedding + Transformer Flow) |
| **Data**             | `the-verdict.txt`              |  Sample Text File Used for Training and Testing                         |


### âš–ï¸ License and Credits
All rights for the original materials belong to their respective creator(s).  
This repository exists for educational demonstration only.  
Please cite or reference the original YouTube playlist when reproducing or sharing.

---

### ğŸ‘©â€ğŸ’» Author / KatkÄ±da Bulunan
**Reyhan HoÅŸavcÄ±**  
FSMVÃœ â€” Computer Engineering PhD Student  
AI Researcher | NLP & Vision-Language Models  
ğŸ”— [GitHub Profile](https://github.com/reyhanhosavci)

---

### ğŸª¶ License
MIT License (for the annotations and added comments only).  
Original code may be subject to separate terms by its author.
